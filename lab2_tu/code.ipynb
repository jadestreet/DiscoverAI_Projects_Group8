{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Custom Environment for Reinforcement Learning**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The code below is taken from Nicholas Renotte's tutorial on how to create Custom environments for reinforcement learning. [Tutorial](https://youtu.be/Mut_u40Sqz4?t=8940), [code on github](https://github.com/nicknochnack/ReinforcementLearningCourse/blob/main/Project%203%20-%20Custom%20Environment.ipynb).\n",
                "\n",
                "You are encouraged to visit the links above and check out the full code. In this lab, you will practice training a model."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**About the problem**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The task is to build an agent that regulates the shower temperature to give the best shower possible every time.\n",
                "\n",
                "Based the activity of other people in the building, the temperature fluctuates randomly. Assuming that our optimal temperature is between 37 and 39 degrees, we want to train an agent to automatically respond to changes in temperature and get it back within the preferred range.\n",
                "\n",
                "Note that the agent does not know the preffered range ahead of time, and should instead learn the types of adjustments it can make to get a reward."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Import libraries**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "# Avoid reinstalling packages that are available on edstem\n",
                "if not os.getenv(\"ED_COURSE_ID\"):\n",
                "    !pip install tensorflow stable_baselines3 torch collections gym box2d-py --user\n",
                "\n",
                "# Import gym libraries\n",
                "import gym \n",
                "from gym import Env # the supperclass to build our own environment\n",
                "# All different types of spaces available in Gym\n",
                "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete \n",
                "\n",
                "# Import helpers\n",
                "import numpy as np\n",
                "import random\n",
                "\n",
                "#Import stable bbaselines libraries\n",
                "from stable_baselines3 import PPO\n",
                "from stable_baselines3.common.vec_env import DummyVecEnv\n",
                "from stable_baselines3.common.evaluation import evaluate_policy"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Inspect types of spaces"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There are four key types of Gym spaces:\n",
                "Box, Discrete, Multibinary and MultiDiscrete.\n",
                "\n",
                "There are two wrapper spaces, Tuple and Dict that help group different spaces together.\n",
                "\n",
                "These spaces can be used to create simple environment, like the shower environment in the following example."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a discrete space\n",
                "disc = Discrete(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "1"
                    },
                    "execution_count": 30,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Sample the discrete space for a value (between 0 and 2)\n",
                "disc.sample()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a box space\n",
                "box = Box(0,1,shape=(3,3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[0.67475665, 0.11705777, 0.15032515],\n       [0.2597542 , 0.93930954, 0.3455979 ],\n       [0.10813219, 0.3114431 , 0.1988852 ]], dtype=float32)"
                    },
                    "execution_count": 32,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#TODO: Sample the box space for a value\n",
                "box.sample()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a tuple space and combine a discrete and box spaces\n",
                "tup = Tuple((Discrete(2), Box(0,100, shape=(1,))))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(1, array([56.62021], dtype=float32))"
                    },
                    "execution_count": 34,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#TODO: Sample the tuple space for a value\n",
                "tup.sample()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a dict space\n",
                "dic = Dict({'height':Discrete(2), \"speed\":Box(0,100, shape=(1,))}).sample()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a multibinary space\n",
                "multibi = MultiBinary(4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([0, 1, 1, 0], dtype=int8)"
                    },
                    "execution_count": 37,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#TODO: Sample the multibinary space for a value\n",
                "multibi.sample()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a multidiscrete space\n",
                "multidi = MultiDiscrete([5,2,2])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([0, 1, 1])"
                    },
                    "execution_count": 39,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#TODO: Sample the multidiscrete space for a value\n",
                "multidi.sample()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Create a custom environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a shower environment class with four key functions\n",
                "class ShowerEnv(Env):\n",
                "    # Define a function to initialize the environment\n",
                "    def __init__(self):\n",
                "        # Define the discrete action space: \n",
                "        # Actions we can take, down, hold, up\n",
                "        self.action_space = Discrete(3)\n",
                "        # Define a temperature range from 0 to 100\n",
                "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
                "        # Set initial state: starting temp is 38 +- 3\n",
                "        self.state = 38 + random.randint(-3,3)\n",
                "        # Set shower length: set to 60 seconds for testing\n",
                "        self.shower_length = 60\n",
                "\n",
                "    # Define the step function for what to do in one action step    \n",
                "    def step(self, action):\n",
                "        # Apply impact of the action on current state\n",
                "        # 0 -1 = -1 temperature\n",
                "        # 1 -1 = 0 \n",
                "        # 2 -1 = 1 temperature \n",
                "        self.state += action -1 \n",
                "        # Reduce shower length by 1 second at each action\n",
                "        self.shower_length -= 1 \n",
                "        \n",
                "        # Calculate reward\n",
                "        # If the temperature is within preferred range, the reward is positive\n",
                "        if self.state \u003e= 37 and self.state \u003c= 39: \n",
                "            reward = 1 \n",
                "        # If the reward is outside of preferred range, the reward is negative \n",
                "        else: \n",
                "            reward = -1 \n",
                "        \n",
                "        # Check if shower is done\n",
                "        if self.shower_length \u003c= 0: \n",
                "            done = True\n",
                "        else:\n",
                "            done = False\n",
                "        \n",
                "        # Set placeholder for info\n",
                "        info = {}\n",
                "        \n",
                "        # Return step information\n",
                "        return self.state, reward, done, info\n",
                "\n",
                "    # For this lab, we will not implement a visualization of the environment\n",
                "    def render(self):\n",
                "        # Implement viz\n",
                "        pass\n",
                "    \n",
                "    # Define function to reset the environment for the next run\n",
                "    def reset(self):\n",
                "        # Reset shower temperature to a random value between 35 and 41\n",
                "        self.state = np.array([38 + random.randint(-3,3)]).astype(float)\n",
                "        # Reset shower time\n",
                "        self.shower_length = 60 \n",
                "        return self.state"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Test the environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/usr/lib/python3.10/site-packages/gym/spaces/box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
                }
            ],
            "source": [
                "# Initialize the environment\n",
                "env=ShowerEnv()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([66.573166], dtype=float32)"
                    },
                    "execution_count": 42,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#TODO: Write code to sample the environment's observation space\n",
                "env.observation_space.sample()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "1"
                    },
                    "execution_count": 43,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#TODO: Write code to sample the environment's action space\n",
                "env.action_space.sample()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([41.])"
                    },
                    "execution_count": 44,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Reset the environment\n",
                "env.reset()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Episode:1 Score:-26\nEpisode:2 Score:-60\nEpisode:3 Score:-14\nEpisode:4 Score:-20\nEpisode:5 Score:24\n"
                }
            ],
            "source": [
                "# Test five episodes of taking random Actions\n",
                "# in the environment\n",
                "episodes = 5\n",
                "for episode in range(1, episodes+1):\n",
                "    state = env.reset()\n",
                "    done = False\n",
                "    score = 0 \n",
                "    \n",
                "    while not done:\n",
                "        env.render()\n",
                "        action = env.action_space.sample()\n",
                "        n_state, reward, done, info = env.step(action)\n",
                "        score+=reward\n",
                "    print('Episode:{} Score:{}'.format(episode, score))\n",
                "    \n",
                "env.close()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Earn Your Wings"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Implement the rest of the reinforcement learning algorithm to train the model using MlpPolicy. Save the training in the log_path defined below, and evaluate the model at the end with render set to False. Add comments in your code to explain each step that you take in your implementation.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Using cpu device\nWrapping the env with a `Monitor` wrapper\nWrapping the env in a DummyVecEnv.\nLogging to ReinforcementLearning/ShowerEnvironment/Training/Logs/PPO_4\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 60       |\n|    ep_rew_mean     | -33.8    |\n| time/              |          |\n|    fps             | 3729     |\n|    iterations      | 1        |\n|    time_elapsed    | 0        |\n|    total_timesteps | 2048     |\n---------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 60          |\n|    ep_rew_mean          | -35.1       |\n| time/                   |             |\n|    fps                  | 2605        |\n|    iterations           | 2           |\n|    time_elapsed         | 1           |\n|    total_timesteps      | 4096        |\n| train/                  |             |\n|    approx_kl            | 0.007121207 |\n|    clip_fraction        | 0.0445      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.09       |\n|    explained_variance   | 0.00107     |\n|    learning_rate        | 0.0003      |\n|    loss                 | 30.3        |\n|    n_updates            | 10          |\n|    policy_gradient_loss | -0.00285    |\n|    value_loss           | 57.3        |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 60          |\n|    ep_rew_mean          | -34.7       |\n| time/                   |             |\n|    fps                  | 2360        |\n|    iterations           | 3           |\n|    time_elapsed         | 2           |\n|    total_timesteps      | 6144        |\n| train/                  |             |\n|    approx_kl            | 0.011775843 |\n|    clip_fraction        | 0.0557      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.1        |\n|    explained_variance   | -0.00255    |\n|    learning_rate        | 0.0003      |\n|    loss                 | 28.1        |\n|    n_updates            | 20          |\n|    policy_gradient_loss | -0.00624    |\n|    value_loss           | 61          |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 60          |\n|    ep_rew_mean          | -34.3       |\n| time/                   |             |\n|    fps                  | 2274        |\n|    iterations           | 4           |\n|    time_elapsed         | 3           |\n|    total_timesteps      | 8192        |\n| train/                  |             |\n|    approx_kl            | 0.010175042 |\n|    clip_fraction        | 0.0455      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.09       |\n|    explained_variance   | -0.000151   |\n|    learning_rate        | 0.0003      |\n|    loss                 | 35.6        |\n|    n_updates            | 30          |\n|    policy_gradient_loss | -0.00235    |\n|    value_loss           | 72.6        |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 60          |\n|    ep_rew_mean          | -30.5       |\n| time/                   |             |\n|    fps                  | 2234        |\n|    iterations           | 5           |\n|    time_elapsed         | 4           |\n|    total_timesteps      | 10240       |\n| train/                  |             |\n|    approx_kl            | 0.014794801 |\n|    clip_fraction        | 0.0827      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.09       |\n|    explained_variance   | -2.21e-05   |\n|    learning_rate        | 0.0003      |\n|    loss                 | 28.7        |\n|    n_updates            | 40          |\n|    policy_gradient_loss | -0.00659    |\n|    value_loss           | 68.2        |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 60          |\n|    ep_rew_mean          | -29.8       |\n| time/                   |             |\n|    fps                  | 2210        |\n|    iterations           | 6           |\n|    time_elapsed         | 5           |\n|    total_timesteps      | 12288       |\n| train/                  |             |\n|    approx_kl            | 0.011485748 |\n|    clip_fraction        | 0.035       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.09       |\n|    explained_variance   | -3.59e-05   |\n|    learning_rate        | 0.0003      |\n|    loss                 | 34.2        |\n|    n_updates            | 50          |\n|    policy_gradient_loss | -0.00152    |\n|    value_loss           | 75.9        |\n-----------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 60           |\n|    ep_rew_mean          | -26.1        |\n| time/                   |              |\n|    fps                  | 2193         |\n|    iterations           | 7            |\n|    time_elapsed         | 6            |\n|    total_timesteps      | 14336        |\n| train/                  |              |\n|    approx_kl            | 0.0060360986 |\n|    clip_fraction        | 0.0277       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -1.09        |\n|    explained_variance   | 2.54e-05     |\n|    learning_rate        | 0.0003       |\n|    loss                 | 42.7         |\n|    n_updates            | 60           |\n|    policy_gradient_loss | -0.00331     |\n|    value_loss           | 82           |\n------------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 60           |\n|    ep_rew_mean          | -24.6        |\n| time/                   |              |\n|    fps                  | 2171         |\n|    iterations           | 8            |\n|    time_elapsed         | 7            |\n|    total_timesteps      | 16384        |\n| train/                  |              |\n|    approx_kl            | 0.0054255994 |\n|    clip_fraction        | 0.0158       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -1.08        |\n|    explained_variance   | 0.00287      |\n|    learning_rate        | 0.0003       |\n|    loss                 | 42.9         |\n|    n_updates            | 70           |\n|    policy_gradient_loss | -0.00274     |\n|    value_loss           | 78.7         |\n------------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 60           |\n|    ep_rew_mean          | -23          |\n| time/                   |              |\n|    fps                  | 2162         |\n|    iterations           | 9            |\n|    time_elapsed         | 8            |\n|    total_timesteps      | 18432        |\n| train/                  |              |\n|    approx_kl            | 0.0105596185 |\n|    clip_fraction        | 0.0499       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -1.06        |\n|    explained_variance   | 0.00681      |\n|    learning_rate        | 0.0003       |\n|    loss                 | 32.1         |\n|    n_updates            | 80           |\n|    policy_gradient_loss | -0.00475     |\n|    value_loss           | 81.6         |\n------------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 60          |\n|    ep_rew_mean          | -21.6       |\n| time/                   |             |\n|    fps                  | 2155        |\n|    iterations           | 10          |\n|    time_elapsed         | 9           |\n|    total_timesteps      | 20480       |\n| train/                  |             |\n|    approx_kl            | 0.009702494 |\n|    clip_fraction        | 0.0743      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.07       |\n|    explained_variance   | -0.0094     |\n|    learning_rate        | 0.0003      |\n|    loss                 | 36.7        |\n|    n_updates            | 90          |\n|    policy_gradient_loss | -0.00553    |\n|    value_loss           | 73.7        |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 60          |\n|    ep_rew_mean          | -24.6       |\n| time/                   |             |\n|    fps                  | 2150        |\n|    iterations           | 11          |\n|    time_elapsed         | 10          |\n|    total_timesteps      | 22528       |\n| train/                  |             |\n|    approx_kl            | 0.007066522 |\n|    clip_fraction        | 0.0226      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.07       |\n|    explained_variance   | 0.000645    |\n|    learning_rate        | 0.0003      |\n|    loss                 | 33.4        |\n|    n_updates            | 100         |\n|    policy_gradient_loss | -0.00174    |\n|    value_loss           | 68.3        |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 60          |\n|    ep_rew_mean          | -20.2       |\n| time/                   |             |\n|    fps                  | 2145        |\n|    iterations           | 12          |\n|    time_elapsed         | 11          |\n|    total_timesteps      | 24576       |\n| train/                  |             |\n|    approx_kl            | 0.009544459 |\n|    clip_fraction        | 0.0186      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.08       |\n|    explained_variance   | 0.00215     |\n|    learning_rate        | 0.0003      |\n|    loss                 | 34.7        |\n|    n_updates            | 110         |\n|    policy_gradient_loss | -0.00141    |\n|    value_loss           | 70.6        |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 60          |\n|    ep_rew_mean          | -22.6       |\n| time/                   |             |\n|    fps                  | 2133        |\n|    iterations           | 13          |\n|    time_elapsed         | 12          |\n|    total_timesteps      | 26624       |\n| train/                  |             |\n|    approx_kl            | 0.005481595 |\n|    clip_fraction        | 0.01        |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.07       |\n|    explained_variance   | 0.00718     |\n|    learning_rate        | 0.0003      |\n|    loss                 | 48.9        |\n|    n_updates            | 120         |\n|    policy_gradient_loss | -0.00107    |\n|    value_loss           | 84.6        |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 60          |\n|    ep_rew_mean          | -19.5       |\n| time/                   |             |\n|    fps                  | 2130        |\n|    iterations           | 14          |\n|    time_elapsed         | 13          |\n|    total_timesteps      | 28672       |\n| train/                  |             |\n|    approx_kl            | 0.011213295 |\n|    clip_fraction        | 0.103       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.08       |\n|    explained_variance   | 0.0366      |\n|    learning_rate        | 0.0003      |\n|    loss                 | 35.2        |\n|    n_updates            | 130         |\n|    policy_gradient_loss | -0.0113     |\n|    value_loss           | 67.1        |\n-----------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 60           |\n|    ep_rew_mean          | -21.5        |\n| time/                   |              |\n|    fps                  | 2127         |\n|    iterations           | 15           |\n|    time_elapsed         | 14           |\n|    total_timesteps      | 30720        |\n| train/                  |              |\n|    approx_kl            | 0.0036298467 |\n|    clip_fraction        | 0.0131       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -1.07        |\n|    explained_variance   | 0.00213      |\n|    learning_rate        | 0.0003       |\n|    loss                 | 35.7         |\n|    n_updates            | 140          |\n|    policy_gradient_loss | -0.00028     |\n|    value_loss           | 78.5         |\n------------------------------------------\n---------------------------------------\n| rollout/                |           |\n|    ep_len_mean          | 60        |\n|    ep_rew_mean          | -22.5     |\n| time/                   |           |\n|    fps                  | 2125      |\n|    iterations           | 16        |\n|    time_elapsed         | 15        |\n|    total_timesteps      | 32768     |\n| train/                  |           |\n|    approx_kl            | 0.0116955 |\n|    clip_fraction        | 0.0793    |\n|    clip_range           | 0.2       |\n|    entropy_loss         | -1.03     |\n|    explained_variance   | -0.00456  |\n|    learning_rate        | 0.0003    |\n|    loss                 | 30.4      |\n|    n_updates            | 150       |\n|    policy_gradient_loss | -0.00561  |\n|    value_loss           | 68.9      |\n---------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 60          |\n|    ep_rew_mean          | -27.6       |\n| time/                   |             |\n|    fps                  | 2123        |\n|    iterations           | 17          |\n|    time_elapsed         | 16          |\n|    total_timesteps      | 34816       |\n| train/                  |             |\n|    approx_kl            | 0.009965831 |\n|    clip_fraction        | 0.0978      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.997      |\n|    explained_variance   | 0.0129      |\n|    learning_rate        | 0.0003      |\n|    loss                 | 32.8        |\n|    n_updates            | 160         |\n|    policy_gradient_loss | -0.00562    |\n|    value_loss           | 77.8        |\n-----------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 60           |\n|    ep_rew_mean          | -30.6        |\n| time/                   |              |\n|    fps                  | 2120         |\n|    iterations           | 18           |\n|    time_elapsed         | 17           |\n|    total_timesteps      | 36864        |\n| train/                  |              |\n|    approx_kl            | 0.0065199835 |\n|    clip_fraction        | 0.0459       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -1.01        |\n|    explained_variance   | -0.0203      |\n|    learning_rate        | 0.0003       |\n|    loss                 | 35.3         |\n|    n_updates            | 170          |\n|    policy_gradient_loss | -0.00268     |\n|    value_loss           | 70.1         |\n------------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 60           |\n|    ep_rew_mean          | -26.7        |\n| time/                   |              |\n|    fps                  | 2116         |\n|    iterations           | 19           |\n|    time_elapsed         | 18           |\n|    total_timesteps      | 38912        |\n| train/                  |              |\n|    approx_kl            | 0.0026258836 |\n|    clip_fraction        | 0.0436       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -1.03        |\n|    explained_variance   | -0.000133    |\n|    learning_rate        | 0.0003       |\n|    loss                 | 36           |\n|    n_updates            | 180          |\n|    policy_gradient_loss | 0.000765     |\n|    value_loss           | 87.1         |\n------------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 60          |\n|    ep_rew_mean          | -25.6       |\n| time/                   |             |\n|    fps                  | 2115        |\n|    iterations           | 20          |\n|    time_elapsed         | 19          |\n|    total_timesteps      | 40960       |\n| train/                  |             |\n|    approx_kl            | 0.009277165 |\n|    clip_fraction        | 0.0561      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.06       |\n|    explained_variance   | 0.00042     |\n|    learning_rate        | 0.0003      |\n|    loss                 | 34.9        |\n|    n_updates            | 190         |\n|    policy_gradient_loss | -0.00396    |\n|    value_loss           | 82.5        |\n-----------------------------------------\n"
                },
                {
                    "data": {
                        "text/plain": "\u003cstable_baselines3.ppo.ppo.PPO at 0x7f6a1c4e0ac0\u003e"
                    },
                    "execution_count": 55,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Define a path for where to output the training log files\n",
                "log_path = os.path.join('ReinforcementLearning/ShowerEnvironment/Training', 'Logs')\n",
                "#build model and start training it\n",
                "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
                "model.learn(total_timesteps=40000)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [],
            "source": [
                "#save and evaluate\n",
                "model.save('PPO')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(-24.0, 54.99090833947008)"
                    },
                    "execution_count": 57,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "evaluate_policy(model, env, n_eval_episodes=10, render=False)"
            ]
        }
    ]
}
